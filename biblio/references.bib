
@article{noauthor_notitle_nodate,
}

@inproceedings{snelson_warped_2003,
	title = {Warped {Gaussian} {Processes}},
	volume = {16},
	url = {https://proceedings.neurips.cc/paper/2003/hash/6b5754d737784b51ec5075c0dc437bf0-Abstract.html},
	abstract = {We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algo- rithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to signiﬁcantly better performance than using a regular GP, or a GP with a ﬁxed transformation.},
	urldate = {2024-02-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Snelson, Edward and Ghahramani, Zoubin and Rasmussen, Carl},
	year = {2003},
	file = {Full Text PDF:/home/liamlatour/Zotero/storage/4LYQP96Z/Snelson et al. - 2003 - Warped Gaussian Processes.pdf:application/pdf},
}

@article{settles_active_2009,
	title = {Active {Learning} {Literature} {Survey}},
	url = {https://burrsettles.com/pub/settles.activelearning.pdf},
	doi = {null},
	abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difﬁcult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.},
	journal = {null},
	author = {Settles, Burr},
	year = {2009},
	pmid = {null},
	pmcid = {null},
}

@article{rasmussen_gaussian_2005,
	title = {Gaussian {Processes} for {Machine} {Learning}},
	doi = {null},
	abstract = {Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.},
	journal = {null},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2005},
	pmid = {null},
	pmcid = {null},
}

@article{jensen_bounded_2013,
	title = {Bounded {Gaussian} process regression},
	doi = {10.1109/mlsp.2013.6661916},
	abstract = {We extend the Gaussian process (GP) framework for bounded regression by introducing two bounded likelihood functions that model the noise on the dependent variable explicitly. This is fundamentally different from the implicit noise assumption in the previously suggested warped GP framework. We approximate the intractable posterior distributions by the Laplace approximation and expectation propagation and show the properties of the models on an artificial example. We finally consider two real-world data sets originating from perceptual rating experiments which indicate a significant gain obtained with the proposed explicit noise-model extension.},
	journal = {International Workshop on Machine Learning for Signal Processing},
	author = {Jensen, Bjørn Sand and Nielsen, Jakob Skov and Nielsen, Jens Brehm and Larsen, Jan},
	year = {2013},
	pmid = {null},
	pmcid = {null},
}

@article{nickisch_approximations_2008,
	title = {Approximations for {Binary} {Gaussian} {Process} {Classification}},
	doi = {null},
	abstract = {We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.},
	journal = {Journal of Machine Learning Research},
	author = {Nickisch, Hannes and Rasmussen, Carl Edward},
	year = {2008},
	pmid = {null},
	pmcid = {null},
}

@article{snelson_warped_2003-1,
	title = {Warped {Gaussian} {Processes}},
	doi = {null},
	abstract = {We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to significantly better performance than using a regular GP, or a GP with a fixed transformation.},
	journal = {Neural Information Processing Systems},
	author = {Snelson, Edward and Ghahramani, Zoubin and Rasmussen, Carl Edward},
	year = {2003},
	pmid = {null},
	pmcid = {null},
}

@article{knudde_active_2019,
	title = {Active {Learning} for {Feasible} {Region} {Discovery}},
	doi = {10.1109/icmla.2019.00106},
	abstract = {Often in the design process of an engineer, the design specifications of the system are not completely known initially. However, usually there are some physical constraints which are already known, corresponding to a region of interest in the design space that is called feasible. These constraints often have no analytical form but need to be characterised based on expensive simulations or measurements. Therefore, it is important that the feasible region can be modeled sufficiently accurate using only a limited amount of samples. This can be solved by using active learning techniques that minimize the amount of samples w.r.t. what we try to model. Most active learning strategies focus on classification models or regression models with classification accuracy and regression accuracy in mind respectively. In this work, regression models of the constraints are used, but only the (in) feasibility is of interest. To tackle this problem, an information-theoretic sampling strategy is constructed to discover these regions. The proposed method is then tested on two synthetic examples and one engineering example and proves to outperform the current state-of-the-art.},
	journal = {International Conference on Machine Learning and Applications},
	author = {Knudde, Nicolas and Couckuyt, Ivo and Shintani, Kohei and Dhaene, Tom},
	year = {2019},
	pmid = {null},
	pmcid = {null},
}

@article{kapoor_active_2007,
	title = {Active {Learning} with {Gaussian} {Processes} for {Object} {Categorization}},
	doi = {10.1109/iccv.2007.4408844},
	abstract = {Discriminative methods for visual object category recognition are typically non-probabilistic, predicting class labels but not directly providing an estimate of uncertainty. Gaussian Processes (GPs) are powerful regression techniques with explicit uncertainty models; we show here how Gaussian Processes with covariance functions defined based on a Pyramid Match Kernel (PMK) can be used for probabilistic object category recognition. The uncertainty model provided by GPs offers confidence estimates at test points, and naturally allows for an active learning paradigm in which points are optimally selected for interactive labeling. We derive a novel active category learning method based on our probabilistic regression model, and show that a significant boost in classification performance is possible, especially when the amount of training data for a category is ultimately very small.},
	journal = {IEEE International Conference on Computer Vision},
	author = {Kapoor, Ashish and Grauman, Kristen and Urtasun, Raquel and Darrell, Trevor},
	year = {2007},
	pmid = {null},
	pmcid = {null},
}

@article{zhao_efficient_2021,
	title = {Efficient {Active} {Learning} for {Gaussian} {Process} {Classification} by {Error} {Reduction}},
	doi = {null},
	abstract = {In this appendix, we provide more detailed descriptions of the phase identiﬁcation example in the introduction section, the pseudocode of active learning algorithms, the tested datasets, as well as additional experimental results.},
	journal = {Neural Information Processing Systems},
	author = {Zhao, Guang and Dougherty, E. and Yoon, Byung-Jun and Alexander, Francis J. and Qian, Xiaoning},
	year = {2021},
	pmid = {null},
	pmcid = {null},
}

@article{ha_effect_2013,
	title = {The effect of sacrificial templates on the pore characteristics of sintered diatomite membranes},
	volume = {121},
	issn = {1348-6535, 1882-0743},
	url = {https://www.jstage.jst.go.jp/article/jcersj2/121/1419/121_JCSJ-P13123/_article},
	doi = {10.2109/jcersj2.121.940},
	language = {en},
	number = {1419},
	urldate = {2024-02-27},
	journal = {Journal of the Ceramic Society of Japan},
	author = {Ha, Jang-Hoon and Oh, Eunji and Song, In-Hyuck},
	year = {2013},
	pages = {940--945},
	file = {Full Text:/home/liamlatour/Zotero/storage/MSUIJ64Y/Ha et al. - 2013 - The effect of sacrificial templates on the pore ch.pdf:application/pdf},
}

@incollection{leriche_sintering_2017,
	title = {Sintering of {Ceramics}},
	isbn = {978-0-12-803581-8},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128035818102887},
	language = {en},
	urldate = {2024-02-27},
	booktitle = {Reference {Module} in {Materials} {Science} and {Materials} {Engineering}},
	publisher = {Elsevier},
	author = {Leriche, Anne and Cambier, Francis and Hampshire, Stuart},
	year = {2017},
	doi = {10.1016/B978-0-12-803581-8.10288-7},
	pages = {B9780128035818102887},
	file = {Submitted Version:/home/liamlatour/Zotero/storage/9JI2EQT4/Leriche et al. - 2017 - Sintering of Ceramics.pdf:application/pdf},
}

@misc{lewis_sequential_1994,
	title = {A {Sequential} {Algorithm} for {Training} {Text} {Classifiers}},
	url = {http://arxiv.org/abs/cmp-lg/9407020},
	abstract = {The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.},
	urldate = {2024-02-27},
	publisher = {arXiv},
	author = {Lewis, David D. and Gale, William A.},
	month = jul,
	year = {1994},
	note = {arXiv:cmp-lg/9407020},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 10 pages, uuencoded, compressed PostScript; Proc. SIGIR-94 LaTex available from lewis@research.att.com},
	file = {arXiv.org Snapshot:/home/liamlatour/Zotero/storage/S5LQWPBH/9407020.html:text/html;Full Text PDF:/home/liamlatour/Zotero/storage/THQEIC7W/Lewis and Gale - 1994 - A Sequential Algorithm for Training Text Classifie.pdf:application/pdf},
}

@incollection{hutchison_multi-class_2006,
	address = {Berlin, Heidelberg},
	title = {Multi-class {Ensemble}-{Based} {Active} {Learning}},
	volume = {4212},
	isbn = {978-3-540-45375-8 978-3-540-46056-5},
	url = {http://link.springer.com/10.1007/11871842_68},
	abstract = {Ensemble-based active learning has been proven to eﬃciently reduce the number of training instances and thus the cost of data acquisition. To determine the utility of a candidate training instance, the disagreement about its class value among the ensemble members is used. While the disagreement for binary classiﬁcation is easily determined using margins, the adaption to multi-class problems is not straightforward and little studied in the literature. In this paper we consider four approaches to measure ensemble disagreement, including margins, uncertainty sampling and entropy, and evaluate them empirically on various ensemble strategies for active learning. We show that margins outperform the other disagreement measures on three of four active learning strategies. Our experiments also show that some active learning strategies are more sensitive to the choice of disagreement measure than others.},
	language = {en},
	urldate = {2024-02-27},
	booktitle = {Machine {Learning}: {ECML} 2006},
	publisher = {Springer Berlin Heidelberg},
	author = {Körner, Christine and Wrobel, Stefan},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Fürnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
	year = {2006},
	doi = {10.1007/11871842_68},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {687--694},
	file = {Körner and Wrobel - 2006 - Multi-class Ensemble-Based Active Learning.pdf:/home/liamlatour/Zotero/storage/3C9T4IYF/Körner and Wrobel - 2006 - Multi-class Ensemble-Based Active Learning.pdf:application/pdf},
}

@article{schein_active_2004,
	title = {{ACTIVE} {LEARNING} {FOR} {LOGISTIC} {REGRESSION}},
	url = {https://www.lri.fr/~sebag/Examens/Active/schein04active.pdf},
	language = {en},
	journal = {Computer and Information Science},
	author = {Schein, Andrew Ian},
	year = {2004},
	file = {Schein - ACTIVE LEARNING FOR LOGISTIC REGRESSION.pdf:/home/liamlatour/Zotero/storage/85PLBKZF/Schein - ACTIVE LEARNING FOR LOGISTIC REGRESSION.pdf:application/pdf},
}

@inproceedings{settles_multiple-instance_2007,
	title = {Multiple-{Instance} {Active} {Learning}},
	volume = {20},
	url = {https://papers.nips.cc/paper_files/paper/2007/hash/a1519de5b5d44b31a01de013b9b51a80-Abstract.html},
	abstract = {In a multiple instance (MI) learning problem, instances are naturally organized into bags and it is the bags, instead of individual instances, that are labeled for training. MI learners assume that every instance in a bag labeled negative is actually negative, whereas at least one instance in a bag labeled positive is actually positive. We present a framework for active learning in the multiple-instance setting. In particular, we consider the case in which an MI learner is allowed to selectively query unlabeled instances in positive bags. This approach is well motivated in domains in which it is inexpensive to acquire bag labels and possible, but expensive, to acquire instance labels. We describe a method for learning from labels at mixed levels of granularity, and introduce two active query selection strategies motivated by the MI setting. Our experiments show that learning from instance labels can significantly improve performance of a basic MI learning algorithm in two multiple-instance domains: content-based image recognition and text classification.},
	urldate = {2024-02-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Settles, Burr and Craven, Mark and Ray, Soumya},
	year = {2007},
	file = {Full Text PDF:/home/liamlatour/Zotero/storage/VHJTTTL8/Settles et al. - 2007 - Multiple-Instance Active Learning.pdf:application/pdf},
}

@inproceedings{cai_active_2014,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Active {Learning} for {Support} {Vector} {Machines} with {Maximum} {Model} {Change}},
	isbn = {978-3-662-44848-9},
	doi = {10.1007/978-3-662-44848-9_14},
	abstract = {Margin-based strategies and model change based strategies represent two important types of strategies for active learning. While margin-based strategies have been dominant for Support Vector Machines (SVMs), most methods are based on heuristics and lack a solid theoretical support. In this paper, we propose an active learning strategy for SVMs based on Maximum Model Change (MMC). The model change is defined as the difference between the current model parameters and the updated parameters obtained with the enlarged training set. Inspired by Stochastic Gradient Descent (SGD) update rule, we measure the change as the gradient of the loss at a candidate point. We analyze the convergence property of the proposed method, and show that the upper bound of label requests made by MMC is smaller than passive learning. Moreover, we connect the proposed MMC algorithm with the widely used simple margin method in order to provide a theoretical justification for margin-based strategies. Extensive experimental results on various benchmark data sets from UCI machine learning repository have demonstrated the effectiveness and efficiency of the proposed method.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Cai, Wenbin and Zhang, Ya and Zhou, Siyuan and Wang, Wenquan and Ding, Chris and Gu, Xiao},
	editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
	year = {2014},
	keywords = {Active Learning, Maximum Model Change, SVMs},
	pages = {211--226},
	file = {Full Text PDF:/home/liamlatour/Zotero/storage/74MRBCLA/Cai et al. - 2014 - Active Learning for Support Vector Machines with M.pdf:application/pdf},
}

@inproceedings{xu_incorporating_2007,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Incorporating {Diversity} and {Density} in {Active} {Learning} for {Relevance} {Feedback}},
	isbn = {978-3-540-71496-5},
	doi = {10.1007/978-3-540-71496-5_24},
	abstract = {Relevance feedback, which uses the terms in relevant documents to enrich the user’s initial query, is an effective method for improving retrieval performance. An associated key research problem is the following: Which documents to present to the user so that the user’s feedback on the documents can significantly impact relevance feedback performance. This paper views this as an active learning problem and proposes a new algorithm which can efficiently maximize the learning benefits of relevance feedback. This algorithm chooses a set of feedback documents based on relevancy, document diversity and document density. Experimental results show a statistically significant and appreciable improvement in the performance of our new approach over the existing active feedback methods.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer},
	author = {Xu, Zuobing and Akella, Ram and Zhang, Yi},
	editor = {Amati, Giambattista and Carpineto, Claudio and Romano, Giovanni},
	year = {2007},
	keywords = {Mean Average Precision, Query Model, Relevance Feedback, Relevance Score, Retrieval Performance},
	pages = {246--257},
}

@inproceedings{brinker_incorporating_2003,
	address = {Washington, DC, USA},
	series = {{ICML}'03},
	title = {Incorporating diversity in active learning with support vector machines},
	isbn = {978-1-57735-189-4},
	abstract = {In many real world applications, active selection of training examples can significantly reduce the number of labelled training examples to learn a classification function. Different strategies in the field of support vector machines have been proposed that iteratively select a single new example from a set of unlabelled examples, query the corresponding class label and then perform retraining of the current classifier. However, to reduce computational time for training, it might be necessary to select batches of new training examples instead of single examples. Strategies for single examples can be extended straightforwardly to select batches by choosing the h {\textgreater} 1 examples that get the highest values for the individual selection criterion. We present a new approach that is especially designed to construct batches and incorporates a diversity measure. It has low computational requirements making it feasible for large scale problems with several thousands of examples. Experimental results indicate that this approach provides a faster method to attain a level of generalization accuracy in terms of the number of labelled examples.},
	urldate = {2024-02-28},
	booktitle = {Proceedings of the {Twentieth} {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {AAAI Press},
	author = {Brinker, Klaus},
	month = aug,
	year = {2003},
	pages = {59--66},
}

@article{kremer_active_2014,
	title = {Active learning with support vector machines},
	volume = {4},
	copyright = {© 2014 John Wiley \& Sons, Ltd},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1132},
	doi = {10.1002/widm.1132},
	abstract = {In machine learning, active learning refers to algorithms that autonomously select the data points from which they will learn. There are many data mining applications in which large amounts of unlabeled data are readily available, but labels (e.g., human annotations or results coming from complex experiments) are costly to obtain. In such scenarios, an active learning algorithm aims at identifying data points that, if labeled and used for training, would most improve the learned model. Labels are then obtained only for the most promising data points. This speeds up learning and reduces labeling costs. Support vector machine (SVM) classifiers are particularly well-suited for active learning due to their convenient mathematical properties. They perform linear classification, typically in a kernel-induced feature space, which makes expressing the distance of a data point from the decision boundary straightforward. Furthermore, heuristics can efficiently help estimate how strongly learning from a data point influences the current model. This information can be used to actively select training samples. After a brief introduction to the active learning problem, we discuss different query strategies for selecting informative data points and review how these strategies give rise to different variants of active learning with SVMs. This article is categorized under: Technologies {\textgreater} Machine Learning},
	language = {en},
	number = {4},
	urldate = {2024-02-28},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Kremer, Jan and Steenstrup Pedersen, Kim and Igel, Christian},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1132},
	pages = {313--326},
	file = {Snapshot:/home/liamlatour/Zotero/storage/JWLI7EW6/widm.html:text/html},
}

@inproceedings{gu_active_2015,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Active {Learning} based on {Random} {Forest} and {Its} {Application} to {Terrain} {Classification}},
	isbn = {978-3-319-08422-0},
	doi = {10.1007/978-3-319-08422-0_41},
	abstract = {In this paper, a novel active learning technique was proposed for solving multiclass classification problem with random forest classifier. By combining uncertainty, density, and diversity criteria, the most informative samples are selected for manually labeling. The uncertainty criterion is implemented by analyzing the difference between the most votes and second most votes from classifier’s output. Samples in dense regions are thought to be more informative than samples in sparse regions. The average distance of a sample to its k-nearest unlabeled neighbors is computed to describe the sample’s density. The distance between a sample and its nearest labeled sample is used to measure the diversity of the sample. The larger the distance is, the less redundancy the sample is. To assess the effectiveness of the proposed method, it was compared with other techniques like traditional active learning based on random forest and SVM. The results of the experiment on terrain classification have demonstrated the effectiveness of the proposed approach.},
	language = {en},
	booktitle = {Progress in {Systems} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Gu, Yingjie and Zydek, Dawid and Jin, Zhong},
	editor = {Selvaraj, Henry and Zydek, Dawid and Chmaj, Grzegorz},
	year = {2015},
	keywords = {Active Learning Algorithm, Current Decision Boundary, Manual Labeling, Maximum Votes, Random Forest},
	pages = {273--278},
}

@inproceedings{settles_analysis_2008,
	address = {Honolulu, Hawaii},
	title = {An {Analysis} of {Active} {Learning} {Strategies} for {Sequence} {Labeling} {Tasks}},
	url = {https://aclanthology.org/D08-1112},
	urldate = {2024-02-28},
	booktitle = {Proceedings of the 2008 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Settles, Burr and Craven, Mark},
	editor = {Lapata, Mirella and Ng, Hwee Tou},
	month = oct,
	year = {2008},
	pages = {1070--1079},
	file = {Full Text PDF:/home/liamlatour/Zotero/storage/7IT6GUN7/Settles and Craven - 2008 - An Analysis of Active Learning Strategies for Sequ.pdf:application/pdf},
}
